{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Dhruv Pithadia\n",
    "\n",
    "Roll No: R013\n",
    "\n",
    "Program: MBA Tech AI\n",
    "\n",
    "Course: Natural Language Processing\n",
    "\n",
    "Topic: Label Encoding, One hot Encoding, Bag Of Words, TF-IDF, Cosine Similarity\n",
    "\n",
    "Contact: pithadia.dhruv@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = '''Artificial intelligence (AI) is a branch of computer science that aims to create machines capable of performing tasks that typically require human intelligence. These tasks include reasoning, learning, problem-solving, perception, and language understanding. AI systems are increasingly used in various applications such as speech recognition, image analysis, and autonomous vehicles. The field of AI encompasses machine learning, neural networks, and natural language processing.\n",
    "'''\n",
    "\n",
    "document2 = '''Machine learning is a subset of artificial intelligence that focuses on developing algorithms and statistical models that enable computers to learn from and make decisions based on data. It involves training models on large datasets to identify patterns and make predictions. Machine learning techniques include supervised learning, unsupervised learning, and reinforcement learning. Applications of machine learning can be found in areas such as recommendation systems, fraud detection, and predictive analytics.  '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_document(document):\n",
    "    \"\"\"\n",
    "    Tokenizes and removes stop words from a document.\n",
    "\n",
    "    Parameters:\n",
    "    document (str): The document text to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of preprocessed tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the document\n",
    "    tokens = word_tokenize(document)\n",
    "    \n",
    "    # Convert tokens to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def compute_cosine_similarity(matrix):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity for a given matrix.\n",
    "\n",
    "    Parameters:\n",
    "    matrix (numpy.ndarray or pandas.DataFrame): The feature matrix.\n",
    "\n",
    "    Returns:\n",
    "    float: Cosine similarity score between the first and second rows.\n",
    "    \"\"\"\n",
    "    cosine_sim = cosine_similarity(matrix)\n",
    "    return cosine_sim[0, 1]\n",
    "\n",
    "def label_encode(tokens, all_tokens):\n",
    "    \"\"\"\n",
    "    Encodes tokens into numerical labels.\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): A list of tokens to be encoded.\n",
    "    all_tokens (list): List of all tokens for creating consistent label mapping.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: An array of encoded labels.\n",
    "    \"\"\"\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_tokens)\n",
    "    encoded_labels = label_encoder.transform(tokens)\n",
    "    return encoded_labels\n",
    "\n",
    "def one_hot_encode(tokens, all_tokens):\n",
    "    \"\"\"\n",
    "    One-hot encodes a list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    tokens (list): A list of tokens to be one-hot encoded.\n",
    "    all_tokens (list): List of all tokens for creating consistent one-hot encoding.\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: An array of one-hot encoded vectors.\n",
    "    \"\"\"\n",
    "    one_hot_encoder = OneHotEncoder(sparse_output=False, categories=[sorted(set(all_tokens))])\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(np.array(tokens).reshape(-1, 1))\n",
    "    return one_hot_encoded\n",
    "\n",
    "def bag_of_words(documents):\n",
    "    \"\"\"\n",
    "    Computes the Bag of Words representation of documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of document texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with BoW features.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(documents)\n",
    "    return X_bow\n",
    "\n",
    "def tfidf(documents):\n",
    "    \"\"\"\n",
    "    Computes the TF-IDF representation of documents.\n",
    "\n",
    "    Parameters:\n",
    "    documents (list): A list of document texts.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with TF-IDF features.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(documents)\n",
    "    return X_tfidf\n",
    "\n",
    "def compare_similarities(doc1, doc2):\n",
    "    \"\"\"\n",
    "    Compares cosine similarities using different feature representations.\n",
    "\n",
    "    Parameters:\n",
    "    doc1 (str): The first document.\n",
    "    doc2 (str): The second document.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with similarity scores for each representation.\n",
    "    \"\"\"\n",
    "    # Preprocess documents\n",
    "    tokens1 = preprocess_document(doc1)\n",
    "    tokens2 = preprocess_document(doc2)\n",
    "    \n",
    "    # Combine tokens from both documents to create a vocabulary\n",
    "    all_tokens = list(set(tokens1 + tokens2))\n",
    "    \n",
    "    # Label Encoding\n",
    "    label_encoded_all = label_encode(all_tokens, all_tokens)\n",
    "    label_encoded_doc1 = label_encode(tokens1, all_tokens)\n",
    "    label_encoded_doc2 = label_encode(tokens2, all_tokens)\n",
    "    \n",
    "    # Create feature matrix for label encoding\n",
    "    label_encoded_docs = np.zeros((2, len(all_tokens)))\n",
    "    for i, doc_encoded in enumerate([label_encoded_doc1, label_encoded_doc2]):\n",
    "        for token in doc_encoded:\n",
    "            if token < len(label_encoded_docs[i]):\n",
    "                label_encoded_docs[i, token] += 1\n",
    "    \n",
    "    # Compute cosine similarity for Label Encoding\n",
    "    label_similarity = compute_cosine_similarity(label_encoded_docs)\n",
    "    \n",
    "    # One-Hot Encoding\n",
    "    one_hot_encoded_all = one_hot_encode(all_tokens, all_tokens)\n",
    "    one_hot_encoded_doc1 = one_hot_encode(tokens1, all_tokens)\n",
    "    one_hot_encoded_doc2 = one_hot_encode(tokens2, all_tokens)\n",
    "    \n",
    "    # Aggregate one-hot encodings\n",
    "    one_hot_encoded_docs = np.zeros((2, one_hot_encoded_all.shape[1]))\n",
    "    for i, one_hot_encoded in enumerate([one_hot_encoded_doc1, one_hot_encoded_doc2]):\n",
    "        one_hot_encoded_docs[i, :] = one_hot_encoded.mean(axis=0)\n",
    "    \n",
    "    # Compute cosine similarity for One-Hot Encoding\n",
    "    one_hot_similarity = compute_cosine_similarity(one_hot_encoded_docs)\n",
    "    \n",
    "    # Bag of Words\n",
    "    bow_df = bag_of_words([doc1, doc2])\n",
    "    bow_similarity = compute_cosine_similarity(bow_df.toarray())\n",
    "    \n",
    "    # TF-IDF\n",
    "    tfidf_df = tfidf([doc1, doc2])\n",
    "    tfidf_similarity = compute_cosine_similarity(tfidf_df.toarray())\n",
    "    \n",
    "    # Compile results\n",
    "    similarities = {\n",
    "        'Label Encoding': label_similarity,\n",
    "        'One-Hot Encoding': one_hot_similarity,\n",
    "        'Bag of Words': bow_similarity,\n",
    "        'TF-IDF': tfidf_similarity\n",
    "    }\n",
    "    \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity using Label Encoding: 0.29311204305478855\n",
      "Cosine Similarity using One-Hot Encoding: 0.2931120430547886\n",
      "Cosine Similarity using Bag of Words: 0.4590638463165382\n",
      "Cosine Similarity using TF-IDF: 0.31818644767206233\n"
     ]
    }
   ],
   "source": [
    "# Compare similarities\n",
    "similarities = compare_similarities(document1, document2)\n",
    "\n",
    "# Print similarity scores for each representation\n",
    "for method, score in similarities.items():\n",
    "    print(f\"Cosine Similarity using {method}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = 'Inflation has increased unemployement'\n",
    "d2 = 'The company has increased it sales'\n",
    "d3 = 'Fear increased his pulse'\n",
    "d = []\n",
    "\n",
    "d.append(d1)\n",
    "d.append(d2)\n",
    "d.append(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Inflation has increased unemployement',\n",
       " 'The company has increased it sales',\n",
       " 'Fear increased his pulse']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t0.5844829010200651\n",
      "  (0, 4)\t0.34520501686496574\n",
      "  (0, 2)\t0.444514311537431\n",
      "  (0, 5)\t0.5844829010200651\n",
      "  (1, 8)\t0.4505040726431979\n",
      "  (1, 6)\t0.4505040726431979\n",
      "  (1, 0)\t0.4505040726431979\n",
      "  (1, 9)\t0.4505040726431979\n",
      "  (1, 4)\t0.2660749625405929\n",
      "  (1, 2)\t0.34261995919180055\n",
      "  (2, 7)\t0.546454011634009\n",
      "  (2, 3)\t0.546454011634009\n",
      "  (2, 1)\t0.546454011634009\n",
      "  (2, 4)\t0.3227445421804912\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_scratch(tokens, all_tokens):\n",
    "    token_to_label = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "    encoded_labels = [token_to_label[token] for token in tokens]\n",
    "    return encoded_labels\n",
    "\n",
    "def one_hot_encode_scratch(tokens, all_tokens):\n",
    "    token_to_index = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "    one_hot_encoded = np.zeros(len(all_tokens))\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            index = token_to_index[token]\n",
    "            one_hot_encoded[index] = 1\n",
    "    return one_hot_encoded\n",
    "\n",
    "def bag_of_words_scratch(documents):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    all_tokens = list(set(token for doc in tokenized_docs for token in doc))\n",
    "    all_tokens.sort()\n",
    "    bow_matrix = np.zeros((len(documents), len(all_tokens)))\n",
    "    for i, tokens in enumerate(tokenized_docs):\n",
    "        for token in tokens:\n",
    "            if token in all_tokens:\n",
    "                index = all_tokens.index(token)\n",
    "                bow_matrix[i, index] += 1\n",
    "    return bow_matrix\n",
    "\n",
    "def tfidf_scratch(documents, output_csv='tfidf_output.csv'):\n",
    "    tokenized_docs = [doc.split() for doc in documents]\n",
    "    all_tokens = list(set(token for doc in tokenized_docs for token in doc))\n",
    "    all_tokens.sort()\n",
    "    tf_matrix = np.zeros((len(documents), len(all_tokens)))\n",
    "    for i, tokens in enumerate(tokenized_docs):\n",
    "        for token in tokens:\n",
    "            if token in all_tokens:\n",
    "                index = all_tokens.index(token)\n",
    "                tf_matrix[i, index] += 1\n",
    "    df = np.sum(tf_matrix > 0, axis=0)\n",
    "    num_docs = len(documents)\n",
    "    idf = np.log(num_docs / (df + 1)) + 1\n",
    "    tfidf_matrix = tf_matrix * idf\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix, columns=all_tokens)\n",
    "    tfidf_df.to_csv(output_csv, index=False)\n",
    "    return tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'ai', 'branch', 'computer', 'science', 'aims', 'create', 'machines', 'capable', 'performing', 'tasks', 'typically', 'require', 'human', 'intelligence', 'tasks', 'include', 'reasoning', 'learning', 'perception', 'language', 'understanding', 'ai', 'systems', 'increasingly', 'used', 'various', 'applications', 'speech', 'recognition', 'image', 'analysis', 'autonomous', 'vehicles', 'field', 'ai', 'encompasses', 'machine', 'learning', 'neural', 'networks', 'natural', 'language', 'processing', 'machine', 'learning', 'subset', 'artificial', 'intelligence', 'focuses', 'developing', 'algorithms', 'statistical', 'models', 'enable', 'computers', 'learn', 'make', 'decisions', 'based', 'data', 'involves', 'training', 'models', 'large', 'datasets', 'identify', 'patterns', 'make', 'predictions', 'machine', 'learning', 'techniques', 'include', 'supervised', 'learning', 'unsupervised', 'learning', 'reinforcement', 'learning', 'applications', 'machine', 'learning', 'found', 'areas', 'recommendation', 'systems', 'fraud', 'detection', 'predictive', 'analytics'] \n",
      "\n",
      "['artificial', 'intelligence', 'ai', 'branch', 'computer', 'science', 'aims', 'create', 'machines', 'capable', 'performing', 'tasks', 'typically', 'require', 'human', 'intelligence', 'tasks', 'include', 'reasoning', 'learning', 'perception', 'language', 'understanding', 'ai', 'systems', 'increasingly', 'used', 'various', 'applications', 'speech', 'recognition', 'image', 'analysis', 'autonomous', 'vehicles', 'field', 'ai', 'encompasses', 'machine', 'learning', 'neural', 'networks', 'natural', 'language', 'processing'] \n",
      "\n",
      "['machine', 'learning', 'subset', 'artificial', 'intelligence', 'focuses', 'developing', 'algorithms', 'statistical', 'models', 'enable', 'computers', 'learn', 'make', 'decisions', 'based', 'data', 'involves', 'training', 'models', 'large', 'datasets', 'identify', 'patterns', 'make', 'predictions', 'machine', 'learning', 'techniques', 'include', 'supervised', 'learning', 'unsupervised', 'learning', 'reinforcement', 'learning', 'applications', 'machine', 'learning', 'found', 'areas', 'recommendation', 'systems', 'fraud', 'detection', 'predictive', 'analytics'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t1 = preprocess_document(document1)\n",
    "t2 = preprocess_document(document2)\n",
    "\n",
    "t = t1+t2\n",
    "\n",
    "document = [document1, document2]\n",
    "\n",
    "print(t,'\\n')\n",
    "print(t1,'\\n')\n",
    "print(t2,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After label encoding[48, 49, 36, 3, 4, 5, 6, 7, 8, 9, 10, 16, 12, 13, 14, 49, 16, 74, 18, 83, 20, 43, 22, 36, 87, 25, 26, 27, 81, 29, 30, 31, 32, 33, 34, 35, 36, 37, 82, 83, 40, 41, 42, 43, 44]\n",
      "After one-hot encoding[0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "After bag of words encoding[[1. 2. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 3. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      "  1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1.\n",
      "  0. 1. 2. 0. 0. 0. 2. 0. 1. 1. 0. 0. 1. 1. 1. 3. 0. 0. 1. 1. 0. 0. 1. 1.\n",
      "  1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 2. 0. 2. 1. 0. 1. 1. 0. 1. 1. 1.]\n",
      " [0. 0. 1. 0. 1. 2. 0. 0. 1. 0. 1. 0. 1. 5. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      "  0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
      "  1. 1. 0. 1. 1. 3. 2. 1. 1. 0. 2. 2. 0. 0. 0. 2. 3. 1. 0. 0. 1. 1. 0. 0.\n",
      "  0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 2. 2. 1. 0. 0. 1. 0. 0. 0.]]\n",
      "After tfidf encoding   (AI)   AI  Applications  Artificial   It  Machine  The  These         a  \\\n",
      "0   1.0  2.0           0.0         1.0  0.0      0.0  1.0    1.0  0.594535   \n",
      "1   0.0  0.0           1.0         0.0  1.0      2.0  0.0    0.0  0.594535   \n",
      "\n",
      "   aims  ...  techniques     that        to  training  typically  \\\n",
      "0   1.0  ...         0.0  1.18907  0.594535       0.0        1.0   \n",
      "1   0.0  ...         1.0  1.18907  1.189070       1.0        0.0   \n",
      "\n",
      "   understanding.  unsupervised  used  various  vehicles.  \n",
      "0             1.0           0.0   1.0      1.0        1.0  \n",
      "1             0.0           1.0   0.0      0.0        0.0  \n",
      "\n",
      "[2 rows x 96 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"After label encoding{label_encode_scratch(t1, t)}\")\n",
    "print(f\"After one-hot encoding{one_hot_encode_scratch(t1, t)}\")\n",
    "print(f\"After bag of words encoding{bag_of_words_scratch(document)}\")\n",
    "print(f\"After tfidf encoding{tfidf_scratch(document)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
