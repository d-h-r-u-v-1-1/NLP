{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Dhruv Pithadia\n",
    "\n",
    "Roll No: R013\n",
    "\n",
    "Program: MBA Tech AI\n",
    "\n",
    "Course: Natural Language Processing\n",
    "\n",
    "Topic: Text Preprocessing\n",
    "\n",
    "Contact: pithadia.dhruv@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everything', 'was', 'going', 'so', 'well', 'until', 'I', 'was', 'accosted', 'by', 'a', 'purple', 'giraffe.']\n"
     ]
    }
   ],
   "source": [
    "#Word Tokenization\n",
    "sentence = 'Everything was going so well until I was accosted by a purple giraffe.'\n",
    "tokenised_Sentence = sentence.split()\n",
    "print(tokenised_Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/dhruvpithadia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Giving directions that the mountains are to the west only works when you can see them.', 'He wasnt bitter that she had moved on but from the radish.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "paragraph = 'Giving directions that the mountains are to the west only works when you can see them. He wasnt bitter that she had moved on but from the radish.'\n",
    "sent_tokens = sent_tokenize(paragraph)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Original tokenised Sentence ['Everything', 'was', 'going', 'so', 'well', 'until', 'I', 'was', 'accosted', 'by', 'a', 'purple', 'giraffe.']\n",
      "Sentence after stopword removal ['Everything', 'going', 'well', 'I', 'accosted', 'purple', 'giraffe.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dhruvpithadia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Stop word removal\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "\n",
    "stop_word_Removed = [ _ for _ in tokenised_Sentence if _ not in stop_words ]\n",
    "print('Original tokenised Sentence', tokenised_Sentence)\n",
    "print('Sentence after stopword removal',stop_word_Removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'very', 'bad', 'idea', 'Why', 'teach', 'your', 'child', 'something', 'that', 'would', 'be', 'extremely', 'unusual', 'in', 'English', 'using', 'all', 'the', 'punctuations', 'By', 'the', 'way', 'punctuation', 'is', 'singular', 'and', 'we', 'say', 'punctuation', 'marks', 'The', 'idea', 'though', 'sounds', 'like', 'overparenting', 'to', 'me', 'Your', 'daughter', 'is', 'probably', 'too', 'young', 'anyway', 'Under', '10']\n"
     ]
    }
   ],
   "source": [
    "#Punctuation Removal\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "punct_sentence = '''\n",
    "This is a very bad idea. Why teach your child something that would be extremely unusual in English? (\"using all the punctuations\"). \n",
    "By the way, punctuation is singular and we say punctuation marks. \n",
    "The idea though sounds like \"over-parenting\" to me. Your daughter is probably too young anyway. Under 10??\n",
    "'''\n",
    "tokenised_sentence = word_tokenize(punct_sentence)\n",
    "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokenised_sentence if re.sub(r'[^\\w\\s]', '', token)]\n",
    "\n",
    "print(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n",
      "winner  :  winner\n",
      "dancer  :  dancer\n",
      "traveller  :  travel\n",
      "teacher  :  teacher\n",
      "waiting  :  wait\n",
      "talking  :  talk\n",
      "running  :  run\n",
      "hiding  :  hide\n",
      "eating  :  eat\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "ps = PorterStemmer()\n",
    " \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\", \"winner\", \"dancer\", \"traveller\", \"teacher\",\"waiting\", \"talking\", \"running\", \"hiding\", \"eating\"]\n",
    " \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/dhruvpithadia/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please install these packages while running the code in the next snippet\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    " \n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Once', 'RB'), ('upon', 'JJ'), ('time', 'NN'), (',', ','), ('small', 'JJ'), (',', ','), ('bustling', 'JJ'), ('village', 'NN'), ('nestled', 'VBD'), ('rolling', 'VBG'), ('hills', 'NNS'), ('sparkling', 'VBG'), ('river', 'NN'), (',', ','), ('lived', 'VBD'), ('curious', 'JJ'), ('young', 'JJ'), ('girl', 'NN'), ('named', 'VBN'), ('Lily', 'NNP'), ('.', '.')]\n",
      "[('Every', 'DT'), ('morning', 'NN'), (',', ','), ('eagerly', 'RB'), ('ran', 'VBD'), ('dewy', 'JJ'), ('meadows', 'NNS'), (',', ','), ('chasing', 'VBG'), ('butterflies', 'NNS'), ('singing', 'VBG'), ('cheerful', 'JJ'), ('songs', 'NNS'), ('.', '.')]\n",
      "[('Her', 'PRP$'), ('loyal', 'JJ'), ('dog', 'NN'), (',', ','), ('Max', 'NNP'), (',', ','), ('always', 'RB'), ('followed', 'VBD'), ('closely', 'RB'), ('behind', 'IN'), (',', ','), ('wagging', 'VBG'), ('tail', 'NN'), ('joy', 'NN'), ('.', '.')]\n",
      "[('The', 'DT'), ('villagers', 'NNS'), ('often', 'RB'), ('watched', 'VBD'), ('fond', 'NN'), ('smiles', 'NNS'), (',', ','), ('admiring', 'VBG'), ('boundless', 'NN'), ('energy', 'NN'), ('adventurous', 'JJ'), ('spirit', 'NN'), ('.', '.')]\n",
      "[('One', 'CD'), ('sunny', 'JJ'), ('afternoon', 'NN'), (',', ','), ('exploring', 'VBG'), ('dense', 'NN'), ('forest', 'JJS'), (',', ','), ('Lily', 'RB'), ('discovered', 'VBD'), ('hidden', 'JJ'), ('cave', 'NN'), ('filled', 'VBN'), ('glimmering', 'NN'), ('treasures', 'NNS'), ('.', '.')]\n",
      "[('Overwhelmed', 'NNP'), ('excitement', 'NN'), (',', ','), ('carefully', 'RB'), ('picked', 'VBD'), ('golden', 'JJ'), ('coin', 'NN'), (',', ','), ('imagining', 'VBG'), ('incredible', 'JJ'), ('stories', 'NNS'), ('could', 'MD'), ('tell', 'VB'), ('.', '.')]\n",
      "[('Her', 'PRP$'), ('heart', 'NN'), ('raced', 'VBD'), ('anticipation', 'NN'), (',', ','), ('dreaming', 'VBG'), ('countless', 'NN'), ('adventures', 'NNS'), ('awaited', 'VBD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Parts of Speech Tagging (P.O.S)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "txt ='''\n",
    "Once upon a time, in a small, bustling village nestled between rolling hills and a sparkling river, there lived a curious young girl named Lily. \n",
    "Every morning, she eagerly ran through the dewy meadows, chasing butterflies and singing cheerful songs. \n",
    "Her loyal dog, Max, always followed closely behind, wagging his tail with joy. \n",
    "The villagers often watched her with fond smiles, admiring her boundless energy and adventurous spirit. \n",
    "One sunny afternoon, while exploring the dense forest, Lily discovered a hidden cave filled with glimmering treasures. \n",
    "Overwhelmed with excitement, she carefully picked up a golden coin, imagining the incredible stories it could tell. \n",
    "Her heart raced with anticipation, dreaming of the countless adventures that awaited her.\n",
    "'''\n",
    " \n",
    "# sent_tokenize is one of instances of \n",
    "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
    " \n",
    "tokenized = sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "     \n",
    "    # Word tokenizers is used to find the words \n",
    "    # and punctuation in a string\n",
    "    wordsList = nltk.word_tokenize(i)\n",
    " \n",
    "    # removing stop words from wordList\n",
    "    wordsList = [w for w in wordsList if not w in stop_words] \n",
    " \n",
    "    #  Using a Tagger. Which is part-of-speech \n",
    "    # tagger or POS-tagger. \n",
    "    tagged = nltk.pos_tag(wordsList)\n",
    " \n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "\n",
    "Stemming is like a quick and dirty way to chop off the end of words to get to the root form. It doesn’t care about the meaning of the word, just about chopping off the ends. For example, “running,” “runner,” and “runs” might all be reduced to “run.” The process is a bit rough and can sometimes produce words that aren’t actual words, like “studies” becoming “studi” or “better” becoming “bett.”\n",
    "\n",
    "**Lemmatization**\n",
    "\n",
    "\n",
    "Lemmatization, on the other hand, is a bit smarter. It looks at the word and its meaning before reducing it to its base form. It uses a dictionary to get the correct root form of the word. So “running” becomes “run,” and “better” becomes “good.” It’s more accurate than stemming but also more complex and slower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 4, 'cat': 2, 'chased': 1, 'mouse': 2, 'but': 1, 'was': 1, 'too': 1, 'quick': 1, 'for': 1}\n"
     ]
    }
   ],
   "source": [
    "#To count frequency of each word in the given document\n",
    "\n",
    "freq_sentence = 'The cat chased the mouse, but the mouse was too quick for the cat.'\n",
    "token_freq_sentence = freq_sentence.split()\n",
    "\n",
    "# Normalize the words to lowercase and remove punctuation\n",
    "import string\n",
    "token_freq_sentence = [word.strip(string.punctuation).lower() for word in token_freq_sentence]\n",
    "\n",
    "# Calculate the frequency\n",
    "word_frequency = {}\n",
    "for word in token_freq_sentence:\n",
    "    if word in word_frequency:\n",
    "        word_frequency[word] += 1\n",
    "    else:\n",
    "        word_frequency[word] = 1\n",
    "\n",
    "print(word_frequency)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "I dove into some cool NLP tasks using NLTK. I played around with changing case, breaking sentences and words apart, getting rid of stop words and punctuation, and even tagging parts of speech. I also tried out stemming and lemmatization, figuring out how they differ. Plus, I wrote my own tokenization code instead of using NLTK’s built-in functions. To top it off, I calculated how often each word appeared in a sentence. All in all, it was a solid hands-on session that gave me a deeper understanding of these key NLP techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
